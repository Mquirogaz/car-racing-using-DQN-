{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.animation as animation\n",
    "import cv2\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess(img):\n",
    "    img = img[:84, 6:90] \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEnv(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        skip_frames=4,\n",
    "        stack_frames=4,\n",
    "        initial_no_op=50,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ImageEnv, self).__init__(env, **kwargs)\n",
    "        self.initial_no_op = initial_no_op\n",
    "        self.skip_frames = skip_frames\n",
    "        self.stack_frames = stack_frames\n",
    "    \n",
    "    def reset(self):\n",
    "        s, info = self.env.reset()\n",
    "        for i in range(self.initial_no_op):\n",
    "            s, r, terminated, truncated, info = self.env.step(0)\n",
    "        s = preprocess(s)\n",
    "        self.stacked_state = np.tile(s, (self.stack_frames, 1, 1))  # [4, 84, 84]\n",
    "        return self.stacked_state, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for _ in range(self.skip_frames):\n",
    "            s, r, terminated, truncated, info = self.env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        s = preprocess(s)\n",
    "\n",
    "        self.stacked_state = np.concatenate((self.stacked_state[1:], s[np.newaxis]), axis=0)\n",
    "\n",
    "        return self.stacked_state, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNActionValue(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, activation=F.relu):\n",
    "        super(CNNActionValue, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(state_dim, 16, kernel_size=8, stride=4)  # [N, 4, 84, 84] -> [N, 16, 20, 20]\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)  # [N, 16, 20, 20] -> [N, 32, 9, 9]\n",
    "        self.in_features = 32 * 9 * 9\n",
    "        self.fc1 = nn.Linear(self.in_features, 256)\n",
    "        self.fc2 = nn.Linear(256, action_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view((-1, self.in_features))\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e5)):\n",
    "        self.max_size = max_size\n",
    "        self.s = np.memmap('B:/VSCODE/s_array.npy', dtype=np.float32, shape=(max_size, *state_dim), mode='w+')\n",
    "        self.a = np.memmap('B:/VSCODE/a_array.npy', dtype=np.int64, shape=(max_size, *action_dim), mode='w+')\n",
    "        self.r = np.memmap('B:/VSCODE/r_array.npy', dtype=np.float32, shape=(max_size, 1), mode='w+')\n",
    "        self.s_prime = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "        self.terminated = np.zeros((max_size, 1), dtype=np.float32)\n",
    "\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def update(self, s, a, r, s_prime, terminated):\n",
    "        self.s[self.ptr] = s\n",
    "        self.a[self.ptr] = a\n",
    "        self.r[self.ptr] = r\n",
    "        self.s_prime[self.ptr] = s_prime\n",
    "        self.terminated[self.ptr] = terminated\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, batch_size)\n",
    "        return (\n",
    "            torch.FloatTensor(self.s[ind]),\n",
    "            torch.FloatTensor(self.a[ind]),\n",
    "            torch.FloatTensor(self.r[ind]),\n",
    "            torch.FloatTensor(self.s_prime[ind]),\n",
    "            torch.FloatTensor(self.terminated[ind]), \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=0.00025,\n",
    "        epsilon=1.0,\n",
    "        epsilon_min=0.1,\n",
    "        gamma=0.99,\n",
    "        batch_size=32,\n",
    "        warmup_steps=5000,\n",
    "        buffer_size=int(1e5),\n",
    "        target_update_interval=10000,\n",
    "    ):\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.target_update_interval = target_update_interval\n",
    "\n",
    "        self.network = CNNActionValue(state_dim[0], action_dim)\n",
    "        self.target_network = CNNActionValue(state_dim[0], action_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.optimizer = torch.optim.RMSprop(self.network.parameters(), lr)\n",
    "\n",
    "        self.buffer = ReplayBuffer(state_dim, (1, ), buffer_size)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "        self.network.to(self.device)\n",
    "        self.target_network.to(self.device)\n",
    "        \n",
    "        self.total_steps = 0\n",
    "        self.epsilon_decay = (epsilon - epsilon_min) / 1e6\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def act(self, x, training=True):\n",
    "        self.network.train(training)\n",
    "        if training and ((np.random.rand() < self.epsilon) or (self.total_steps < self.warmup_steps)):\n",
    "            a = np.random.randint(0, self.action_dim)\n",
    "        else:\n",
    "            x = torch.from_numpy(x).float().unsqueeze(0).to(self.device)\n",
    "            q = self.network(x)\n",
    "            a = torch.argmax(q).item()\n",
    "        return a\n",
    "    \n",
    "    def learn(self):\n",
    "        s, a, r, s_prime, terminated = map(lambda x: x.to(self.device), self.buffer.sample(self.batch_size))\n",
    "        \n",
    "        next_q = self.target_network(s_prime).detach()\n",
    "        td_target = r + (1. - terminated) * self.gamma * next_q.max(dim=1, keepdim=True).values\n",
    "        loss = F.mse_loss(self.network(s).gather(1, a.long()), td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        result = {\n",
    "            'total_steps': self.total_steps,\n",
    "            'value_loss': loss.item()\n",
    "        }\n",
    "        return result\n",
    "    \n",
    "    def process(self, transition):\n",
    "        result = {}\n",
    "        self.total_steps += 1\n",
    "        self.buffer.update(*transition)\n",
    "\n",
    "        if self.total_steps > self.warmup_steps:\n",
    "            result = self.learn()\n",
    "            \n",
    "        if self.total_steps % self.target_update_interval == 0:\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.epsilon -= self.epsilon_decay\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CarRacing-v2', continuous=False)\n",
    "env = ImageEnv(env)\n",
    "\n",
    "max_steps = int(2e3)\n",
    "eval_interval = 10000\n",
    "state_dim = (4, 84, 84)\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQN(state_dim, action_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(n_evals=5):\n",
    "    eval_env = gym.make('CarRacing-v2', continuous=False)\n",
    "    eval_env = ImageEnv(eval_env)\n",
    "    \n",
    "    scores = 0\n",
    "    for i in range(n_evals):\n",
    "        (s, _), done, ret = eval_env.reset(), False, 0\n",
    "        while not done:\n",
    "            a = agent.act(s, training=False)\n",
    "            s_prime, r, terminated, truncated, info = eval_env.step(a)\n",
    "            s = s_prime\n",
    "            ret += r\n",
    "            done = terminated or truncated\n",
    "        scores += ret\n",
    "    return np.round(scores / n_evals, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'Step': [], 'AvgReturn': []}\n",
    "\n",
    "(s, _) = env.reset()\n",
    "while True:\n",
    "    a = agent.act(s)\n",
    "    s_prime, r, terminated, truncated, info = env.step(a)\n",
    "    result = agent.process((s, a, r, s_prime, terminated))  # You can track q-losses over training from `result` variable.\n",
    "    \n",
    "    s = s_prime\n",
    "    if terminated or truncated:\n",
    "        s, _ = env.reset()\n",
    "        \n",
    "    if agent.total_steps % eval_interval == 0:\n",
    "        ret = evaluate()\n",
    "        history['Step'].append(agent.total_steps)\n",
    "        history['AvgReturn'].append(ret)\n",
    "        \n",
    "        clear_output()\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(history['Step'], history['AvgReturn'], 'r-')\n",
    "        plt.xlabel('Step', fontsize=16)\n",
    "        plt.ylabel('AvgReturn', fontsize=16)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n",
    "        \n",
    "        torch.save(agent.network.state_dict(), 'dqn.pt')\n",
    "    \n",
    "    if agent.total_steps > max_steps:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
